{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a7cc1d-75c7-4541-aa29-ab706de095e5",
   "metadata": {},
   "source": [
    "# üè• Diabetes Prediction with Amazon SageMaker\n",
    "\n",
    "## Workshop Context\n",
    "\n",
    "You are an **ML Engineer** who received a diabetes prediction dataset from the Data Science team through Amazon DataZone. The data was processed from FHIR healthcare records stored in AWS HealthLake.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. Understand healthcare ML workflows with imbalanced data\n",
    "2. Perform exploratory data analysis on clinical features\n",
    "3. Train a baseline model locally using scikit-learn\n",
    "4. Scale training using Amazon SageMaker Training Jobs\n",
    "5. Deploy a model to a SageMaker endpoint for real-time predictions\n",
    "6. Understand when to use local vs SageMaker training\n",
    "\n",
    "## üìä Dataset Overview\n",
    "\n",
    "**Features:**\n",
    "- `Glucose`: Blood glucose level (mg/dL)\n",
    "- `Hemoglobin A1c/Hemoglobin.total in Blood`: HbA1c percentage\n",
    "- `Body Mass Index`: BMI (kg/m¬≤)\n",
    "- `Body Weight`: Weight in kg\n",
    "- `label_diabetes_onset`: Target (0=No diabetes, 1=Diabetes)\n",
    "\n",
    "**Clinical Context:**\n",
    "- HbA1c ‚â• 6.5% indicates diabetes\n",
    "- Fasting glucose ‚â• 126 mg/dL indicates diabetes\n",
    "- Dataset has severe class imbalance (~1.2% diabetes cases)\n",
    "\n",
    "**Expected Duration:** 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "b8e05433-15a9-4aae-992f-5cb3004f1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, f1_score, precision_recall_curve\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee369b-bef7-4a3b-8cdb-005642d4d722",
   "metadata": {},
   "source": [
    "# Section 1: Data Loading\n",
    "\n",
    "We'll load the diabetes dataset from S3. This data was published by the Data Science team through Amazon DataZone after processing FHIR data with AWS Glue.\n",
    "\n",
    "**What happens here:**\n",
    "- Connect to S3 and load Parquet files\n",
    "- Parquet format is efficient for large datasets (columnar storage, compression)\n",
    "- Display first rows to verify structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "8312fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.athena\n",
    "SELECT * FROM <your_glue_database>.diabetes_ml_dataset;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "2ec2195e-c3fa-4931-9e92-6612d0b24f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Replace with your S3 path\n",
    "s3_location = 's3://your-bucket/path-to-diabetes-dataset/'\n",
    "\n",
    "# Read data directly\n",
    "df = pd.read_parquet(s3_location)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c426b-fb6c-4d9c-8739-9d2cd09387d1",
   "metadata": {},
   "source": [
    "# Section 2: Exploratory Data Analysis\n",
    "\n",
    "## 2.1 Class Distribution Analysis\n",
    "\n",
    "Understanding the balance between diabetes and non-diabetes cases is crucial. In healthcare, diseases are often rare, creating **class imbalance** challenges.\n",
    "\n",
    "**Why this matters:**\n",
    "- Severe imbalance affects model training\n",
    "- We need special techniques (class weighting, resampling)\n",
    "- Evaluation metrics must account for imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "94e919f5-f373-4d3d-a875-59265af4827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class distribution\n",
    "diabetes_count = df['label_diabetes_onset'].sum()\n",
    "total_count = len(df)\n",
    "non_diabetes_count = total_count - diabetes_count\n",
    "\n",
    "diabetes_pct = (diabetes_count / total_count) * 100\n",
    "non_diabetes_pct = (non_diabetes_count / total_count) * 100\n",
    "\n",
    "print(\"üè• CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Patients: {total_count:,}\")\n",
    "print(f\"\\n‚úÖ Non-Diabetes (Class 0): {non_diabetes_count:,} ({non_diabetes_pct:.2f}%)\")\n",
    "print(f\"‚ö†Ô∏è  Diabetes (Class 1):     {diabetes_count:,} ({diabetes_pct:.2f}%)\")\n",
    "print(f\"\\nüìä Imbalance Ratio: {non_diabetes_count/diabetes_count:.1f}:1\")\n",
    "\n",
    "print(\"\\n‚öïÔ∏è CLINICAL INSIGHT:\")\n",
    "print(\"This severe class imbalance (~80:1) is typical in disease prediction.\")\n",
    "print(\"We'll use class weighting to handle this during training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05385364-0a67-41a9-a9e2-f30f2cb0f631",
   "metadata": {},
   "source": [
    "## 2.2 Visualize Class Distribution\n",
    "\n",
    "Visual representation helps understand the scale of imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "1c4f73b0-3700-4102-a67d-67431b3f7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "ax1 = axes[0]\n",
    "counts = [non_diabetes_count, diabetes_count]\n",
    "labels = ['No Diabetes', 'Diabetes']\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "bars = ax1.bar(labels, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Number of Patients', fontsize=12)\n",
    "ax1.set_title('Class Distribution (Absolute Counts)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height):,}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "ax2 = axes[1]\n",
    "ax2.pie([non_diabetes_pct, diabetes_pct], labels=labels, autopct='%1.2f%%',\n",
    "        colors=colors, startangle=90, explode=(0, 0.1))\n",
    "ax2.set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9192d9-fed9-456e-9a33-fb08f165e261",
   "metadata": {},
   "source": [
    "## 2.3 Missing Values Analysis\n",
    "\n",
    "Missing data in healthcare can occur for many reasons:\n",
    "- Tests not ordered for certain patients\n",
    "- Lab results pending or lost\n",
    "- Data integration issues\n",
    "\n",
    "Understanding missingness patterns helps us decide on handling strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "6c8c7898-e4cd-4784-9229-570e7679a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "missing_info = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing Count': missing_values.values,\n",
    "    'Missing %': missing_percentage.values\n",
    "})\n",
    "\n",
    "missing_info = missing_info.sort_values('Missing %', ascending=False)\n",
    "missing_info = missing_info[missing_info['Missing Count'] > 0]\n",
    "\n",
    "print(\"üîç MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(missing_info) > 0:\n",
    "    print(missing_info.to_string(index=False))\n",
    "    \n",
    "    total_cells = np.product(df.shape)\n",
    "    total_missing = missing_values.sum()\n",
    "    overall_pct = (total_missing / total_cells) * 100\n",
    "    \n",
    "    print(f\"\\nüìä SUMMARY:\")\n",
    "    print(f\"Total cells: {total_cells:,}\")\n",
    "    print(f\"Total missing: {total_missing:,}\")\n",
    "    print(f\"Overall missing: {overall_pct:.2f}%\")\n",
    "    \n",
    "    print(\"\\nüí° DECISION:\")\n",
    "    print(\"Creatinine columns have >60% missing values ‚Üí DROP them\")\n",
    "    print(\"Other features have <60% missing ‚Üí Keep and handle during preprocessing\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d56aa6-6e22-4a38-9d8d-9f67ba5a42c3",
   "metadata": {},
   "source": [
    "## 2.4 Drop High-Missing Columns\n",
    "\n",
    "**Decision:** Drop Creatinine columns due to excessive missing values (>60%).\n",
    "\n",
    "**Rationale:**\n",
    "- Too many missing values make imputation unreliable\n",
    "- Creatinine is primarily a kidney function marker\n",
    "- We still have strong diabetes indicators (Glucose, HbA1c, BMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "b4c6b4b7-a30e-4255-aa92-eb5855c22d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop creatinine columns\n",
    "columns_to_drop = [col for col in df.columns if 'creatinine' in col.lower()]\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"‚úÖ Dropped {len(columns_to_drop)} columns\")\n",
    "print(f\"üìä New shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d54592a-c429-40e5-b95d-85cdef0c360e",
   "metadata": {},
   "source": [
    "# Section 3: Local Model Training (Baseline)\n",
    "\n",
    "Now we'll train a baseline model locally using HistGradientBoostingClassifier. This gives us a performance benchmark before moving to SageMaker.\n",
    "\n",
    "**Why HistGradientBoostingClassifier?**\n",
    "- Handles missing values natively\n",
    "- Fast training on large datasets\n",
    "- Built-in class weighting for imbalanced data\n",
    "- Similar performance to XGBoost\n",
    "\n",
    "**Training Strategy:**\n",
    "- 5-fold stratified cross-validation\n",
    "- Class weighting to handle imbalance (80:1 ratio)\n",
    "- Optimize threshold using precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "84d05d54-562e-4585-95da-cc02751b4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def train_local_model(df):\n",
    "    \"\"\"Train baseline model locally with cross-validation\"\"\"\n",
    "    \n",
    "    # Define features\n",
    "    features = [col for col in df.columns if col not in ['patient_id', 'label_diabetes_onset']]\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df[features].apply(pd.to_numeric, errors='coerce').values\n",
    "    y = df['label_diabetes_onset'].values\n",
    "    \n",
    "    # Calculate class weights\n",
    "    n_neg = np.sum(y == 0)\n",
    "    n_pos = np.sum(y == 1)\n",
    "    weight_ratio = n_neg / n_pos\n",
    "    \n",
    "    print(\"üöÄ TRAINING LOCAL MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Features: {len(features)}\")\n",
    "    print(f\"Samples: {len(y):,}\")\n",
    "    print(f\"Class distribution: {n_neg:,} negative, {n_pos:,} positive\")\n",
    "    print(f\"Weight ratio: {weight_ratio:.2f}\")\n",
    "    print()\n",
    "    \n",
    "    # 5-fold cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(cv.split(X, y), 1):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Initialize model with class weights\n",
    "        model = HistGradientBoostingClassifier(\n",
    "            random_state=42,\n",
    "            max_iter=200,\n",
    "            class_weight={0: 1., 1: weight_ratio},\n",
    "            learning_rate=0.1\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Find optimal threshold using precision-recall curve\n",
    "        precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "        f1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1])\n",
    "        optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        \n",
    "        # Make predictions with optimal threshold\n",
    "        y_pred = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        fold_f1 = f1_score(y_test, y_pred)\n",
    "        fold_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        fold_precision = precision_score(y_test, y_pred)\n",
    "        fold_recall = recall_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"=== Fold {fold} ===\")\n",
    "        print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "        print(f\"Precision: {fold_precision:.4f}\")\n",
    "        print(f\"Recall: {fold_recall:.4f}\")\n",
    "        print(f\"F1-Score: {fold_f1:.4f}\")\n",
    "        print(f\"ROC-AUC: {fold_roc_auc:.4f}\")\n",
    "        print()\n",
    "        \n",
    "        results.append({\n",
    "            'f1': fold_f1,\n",
    "            'roc_auc': fold_roc_auc,\n",
    "            'threshold': optimal_threshold,\n",
    "            'precision': fold_precision,\n",
    "            'recall': fold_recall\n",
    "        })\n",
    "    \n",
    "    # Overall results\n",
    "    print(\"=\" * 60)\n",
    "    print(\"=== OVERALL RESULTS ===\")\n",
    "    print(f\"Average Precision: {np.mean([r['precision'] for r in results]):.4f}\")\n",
    "    print(f\"Average Recall: {np.mean([r['recall'] for r in results]):.4f}\")\n",
    "    print(f\"Average F1-Score: {np.mean([r['f1'] for r in results]):.4f}\")\n",
    "    print(f\"Average ROC-AUC: {np.mean([r['roc_auc'] for r in results]):.4f}\")\n",
    "    print(f\"Average Optimal Threshold: {np.mean([r['threshold'] for r in results]):.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Train the model\n",
    "local_results = train_local_model(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60365ef8-9846-4909-a53b-6994a2efb045",
   "metadata": {},
   "source": [
    "## Interpreting the Results\n",
    "\n",
    "**What these metrics mean:**\n",
    "\n",
    "- **ROC-AUC ~0.9798**: Excellent! The model has strong ability to distinguish diabetes from non-diabetes cases\n",
    "- **F1-Score ~0.8203**: Strong balance between precision and recall\n",
    "- **Optimal Threshold ~0.7971**: Higher than default 0.5 due to class imbalance (80:1 ratio)\n",
    "\n",
    "**Clinical Implications:**\n",
    "\n",
    "Based on your results:\n",
    "- **Recall ~76.42%**: Model catches about 3 out of 4 diabetes cases\n",
    "- **Precision ~88.81%**: When model predicts diabetes, it's correct 89% of the time\n",
    "- **False Negatives**: Missing ~24% of diabetes cases - requires clinical backup screening\n",
    "\n",
    "**Trade-off Decision:**\n",
    "- **Lower threshold (0.7760)** ‚Üí Catch more cases (~79% recall) but more false alarms (~84% precision)\n",
    "- **Higher threshold (0.8253)** ‚Üí Fewer false alarms (~92% precision) but miss more cases (~74% recall)\n",
    "- **For screening**: Prefer higher recall (lower threshold ~0.7760) to catch more cases\n",
    "- **For diagnosis**: Prefer higher precision (higher threshold ~0.8253) to reduce false positives\n",
    "\n",
    "**Clinical Context:**\n",
    "- The 76% recall means this model is best used as a screening tool, not a diagnostic tool\n",
    "- The 24% missed cases highlight the importance of combining model predictions with clinical judgment\n",
    "- High precision (89%) means positive predictions are reliable and warrant follow-up testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df85a5eb-9c33-490f-b2f3-a2d9048d1659",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Optional Section Notice\n",
    "\n",
    "**Sections 4 onwards (SageMaker Training Jobs + Endpoint Deployment) are optional.**\n",
    "\n",
    "These sections demonstrate:\n",
    "- Scaling ML training using Amazon SageMaker Training Jobs\n",
    "- Deploying models to SageMaker endpoints for real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff4871-b156-409e-9eff-be253c1bdcc0",
   "metadata": {},
   "source": [
    "# Section 4: SageMaker Training Job\n",
    "\n",
    "Now let's scale our training using Amazon SageMaker. This provides:\n",
    "\n",
    "**Benefits of SageMaker Training:**\n",
    "- ‚úÖ Managed infrastructure (no server management)\n",
    "- ‚úÖ Automatic experiment tracking\n",
    "- ‚úÖ Easy to scale (change instance types)\n",
    "- ‚úÖ Cost-effective (pay per minute)\n",
    "- ‚úÖ Model versioning and registry\n",
    "- ‚úÖ Production-ready artifacts\n",
    "\n",
    "**When to use SageMaker vs Local:**\n",
    "- **Local**: Quick experiments, small datasets, prototyping\n",
    "- **SageMaker**: Production models, large datasets, team collaboration, reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "19f2f6fb-3360-4b81-b441-f7a021d4f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'diabetes-prediction'\n",
    "\n",
    "print(\"üîß SAGEMAKER CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"S3 Bucket: {bucket}\")\n",
    "print(f\"Region: {sagemaker_session.boto_region_name}\")\n",
    "print(f\"Prefix: {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94bb4ae-9d34-4250-8ff2-b4cedd231dc1",
   "metadata": {},
   "source": [
    "## 4.1 Prepare Data for SageMaker\n",
    "\n",
    "SageMaker expects data in CSV format with:\n",
    "- First column: Target variable\n",
    "- Remaining columns: Features\n",
    "- No headers\n",
    "\n",
    "We'll split the data, save to CSV, and upload to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "54776bc4-2794-462d-98d2-70bc8edd2100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "features = [col for col in df.columns if col not in ['patient_id', 'label_diabetes_onset']]\n",
    "\n",
    "# Prepare features and target\n",
    "X = df[features].apply(pd.to_numeric, errors='coerce')\n",
    "y = df['label_diabetes_onset']\n",
    "\n",
    "# Remove rows with NaN\n",
    "mask = ~X.isna().any(axis=1)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "print(f\"üìä Data after removing NaN: {len(X):,} samples\")\n",
    "\n",
    "# Split data (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")\n",
    "\n",
    "# Create CSV files (target first, then features, no headers)\n",
    "train_data = pd.concat([y_train.reset_index(drop=True), X_train.reset_index(drop=True)], axis=1)\n",
    "test_data = pd.concat([y_test.reset_index(drop=True), X_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "train_data.to_csv('train.csv', index=False, header=False)\n",
    "test_data.to_csv('test.csv', index=False, header=False)\n",
    "\n",
    "# Upload to S3\n",
    "train_s3 = sagemaker_session.upload_data('train.csv', bucket=bucket, key_prefix=f'{prefix}/train')\n",
    "test_s3 = sagemaker_session.upload_data('test.csv', bucket=bucket, key_prefix=f'{prefix}/test')\n",
    "\n",
    "print(f\"\\n‚úÖ Training data uploaded to: {train_s3}\")\n",
    "print(f\"‚úÖ Test data uploaded to: {test_s3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beec00b7-b4b9-47ae-8464-f225da582215",
   "metadata": {},
   "source": [
    "## 4.2 Create Training Script\n",
    "\n",
    "We need to create a Python script that SageMaker will execute on the training instance. This script:\n",
    "- Loads data from SageMaker's input channels\n",
    "- Trains the model\n",
    "- Saves the model to SageMaker's model directory\n",
    "\n",
    "The `%%writefile` magic command creates the file in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "248dff51-ea27-404b-b4e7-291bce7c5e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_sagemaker.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score\n",
    "import joblib\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parse hyperparameters\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--max-iter', type=int, default=200)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.1)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    # Load training data\n",
    "    train_path = '/opt/ml/input/data/train'\n",
    "    train_files = [os.path.join(train_path, f) for f in os.listdir(train_path)]\n",
    "    train_data = pd.concat([pd.read_csv(f, header=None) for f in train_files])\n",
    "    \n",
    "    X_train = train_data.iloc[:, 1:].values\n",
    "    y_train = train_data.iloc[:, 0].values\n",
    "    \n",
    "    # Calculate class weights\n",
    "    n_neg = np.sum(y_train == 0)\n",
    "    n_pos = np.sum(y_train == 1)\n",
    "    weight_ratio = n_neg / n_pos\n",
    "    \n",
    "    print(f\"Training samples: {len(y_train)}\")\n",
    "    print(f\"Class distribution: {n_neg} negative, {n_pos} positive\")\n",
    "    print(f\"Weight ratio: {weight_ratio:.2f}\")\n",
    "\n",
    "    # Create sample weights\n",
    "    sample_weights = np.where(y_train == 1, weight_ratio, 1.0) \n",
    "    \n",
    "    # Train model\n",
    "    model = HistGradientBoostingClassifier(\n",
    "        random_state=42,\n",
    "        max_iter=args.max_iter,\n",
    "        learning_rate=args.learning_rate  \n",
    "    )\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    model.fit(X_train, y_train, sample_weight=sample_weights)  \n",
    "\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    # Load test data and evaluate\n",
    "    test_path = '/opt/ml/input/data/test'\n",
    "    test_files = [os.path.join(test_path, f) for f in os.listdir(test_path)]\n",
    "    test_data = pd.concat([pd.read_csv(f, header=None) for f in test_files])\n",
    "    \n",
    "    X_test = test_data.iloc[:, 1:].values\n",
    "    y_test = test_data.iloc[:, 0].values\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(\"\\n=== Model Performance ===\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join('/opt/ml/model', 'model.joblib')\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"\\nModel saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a58401a-3096-4e8f-a711-6c829df2e641",
   "metadata": {},
   "source": [
    "## 4.3 Launch SageMaker Training Job\n",
    "\n",
    "Now we'll configure and launch the training job. SageMaker will:\n",
    "1. Spin up the specified instance type\n",
    "2. Copy our training script and data\n",
    "3. Execute the training\n",
    "4. Save the model artifacts to S3\n",
    "5. Shut down the instance\n",
    "\n",
    "**Instance Selection:**\n",
    "- `ml.m5.xlarge`: Good balance of CPU/memory for this dataset\n",
    "- Training time: ~5-10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "1d6ba6c7-4dd3-4dad-a92e-c6e99b4893b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the estimator\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point='train_sagemaker.py',\n",
    "    role=role,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    framework_version='1.0-1',\n",
    "    py_version='py3',\n",
    "    hyperparameters={\n",
    "        'max-iter': 200,\n",
    "        'learning-rate': 0.1\n",
    "    },\n",
    "    base_job_name='diabetes-training'\n",
    ")\n",
    "\n",
    "print(\"üöÄ Launching SageMaker Training Job...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Instance type: ml.m5.xlarge\")\n",
    "print(f\"Framework: scikit-learn 1.0-1\")\n",
    "print(f\"Hyperparameters: max-iter=200, learning-rate=0.1\")\n",
    "print(\"\\nThis will take ~5-10 minutes...\")\n",
    "\n",
    "# Launch training job\n",
    "sklearn_estimator.fit({'train': train_s3, 'test': test_s3}, wait=True)\n",
    "\n",
    "print(\"\\n‚úÖ Training job completed!\")\n",
    "print(f\"Model artifacts saved to: {sklearn_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8816116c-2607-49cf-85a6-4f12c227661c",
   "metadata": {},
   "source": [
    "# Section 5: Model Deployment\n",
    "\n",
    "Now let's deploy our trained model to a SageMaker endpoint for real-time predictions.\n",
    "\n",
    "**What is a SageMaker Endpoint?**\n",
    "- A managed HTTPS endpoint for real-time inference\n",
    "- Auto-scaling based on traffic\n",
    "- Built-in monitoring and logging\n",
    "- A/B testing capability\n",
    "\n",
    "**Deployment Process:**\n",
    "1. SageMaker creates the endpoint configuration\n",
    "2. Deploys the model to the specified instance\n",
    "3. Endpoint becomes available for predictions\n",
    "\n",
    "**Instance Selection:**\n",
    "- `ml.t2.medium`: Cost-effective for low-traffic endpoints\n",
    "- For production: Use `ml.m5.large` or larger with auto-scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a5022-fdde-4621-9a29-f893c5e8d489",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T12:33:13.472931Z",
     "iopub.status.busy": "2025-10-23T12:33:13.472408Z",
     "iopub.status.idle": "2025-10-23T12:33:13.484494Z",
     "shell.execute_reply": "2025-10-23T12:33:13.483505Z",
     "shell.execute_reply.started": "2025-10-23T12:33:13.472904Z"
    }
   },
   "source": [
    "## Create the Inference Script\n",
    "\n",
    "The training script (`train_sagemaker.py`) only handles model training. For deployment, SageMaker needs to know:\n",
    "- How to load the saved model\n",
    "- How to process incoming prediction requests\n",
    "- How to return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "266d4821-84e4-41c5-abff-1c66026eb54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference.py\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO  # ‚Üê ADD THIS LINE\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load the model from the model_dir\"\"\"\n",
    "    import os\n",
    "    model_path = os.path.join(model_dir, 'model.joblib')\n",
    "    model = joblib.load(model_path)\n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"Parse input data for predictions\"\"\"\n",
    "    if request_content_type == 'text/csv':\n",
    "        # Parse CSV input\n",
    "        input_data = pd.read_csv(StringIO(request_body), header=None)\n",
    "        return input_data.values\n",
    "    elif request_content_type == 'application/json':\n",
    "        # Parse JSON input\n",
    "        import json\n",
    "        input_data = json.loads(request_body)\n",
    "        return np.array(input_data)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Make predictions using the loaded model\"\"\"\n",
    "    predictions = model.predict_proba(input_data)[:, 1]\n",
    "    return (predictions >= 0.82).astype(int)\n",
    "\n",
    "'''def predict_fn(input_data, model):\n",
    "    \"\"\"Make predictions using the loaded model\"\"\"\n",
    "    predictions = model.predict(input_data) \n",
    "    return predictions'''\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    \"\"\"Format the prediction output\"\"\"\n",
    "    if content_type == 'text/csv':  \n",
    "        return ','.join(str(int(x)) for x in prediction), content_type\n",
    "    elif content_type == 'application/json':\n",
    "        import json\n",
    "        return json.dumps(prediction.tolist()), content_type\n",
    "    else:\n",
    "        return str(prediction), content_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9f8e7-f3b8-4e95-b7c0-da7682135c35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T12:36:53.570849Z",
     "iopub.status.busy": "2025-10-23T12:36:53.570287Z",
     "iopub.status.idle": "2025-10-23T12:36:53.578586Z",
     "shell.execute_reply": "2025-10-23T12:36:53.577512Z",
     "shell.execute_reply.started": "2025-10-23T12:36:53.570821Z"
    }
   },
   "source": [
    "## Deploy the Model with Inference Script\n",
    "\n",
    "**What happens during deployment:**\n",
    "\n",
    "1. SageMaker creates a container with scikit-learn\n",
    "2. Downloads your model artifacts from S3\n",
    "3. Loads your inference script\n",
    "4. Calls `model_fn` to load the model\n",
    "5. Starts a web server to handle prediction requests\n",
    "6. Runs health checks (pings the `/ping` endpoint)\n",
    "7. Marks endpoint as \"InService\" when ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "9c1ba260-358e-42a1-9030-07ff443d7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn import SKLearnModel\n",
    "\n",
    "# Create model from training artifacts\n",
    "model = SKLearnModel(\n",
    "    model_data=sklearn_estimator.model_data,\n",
    "    role=role,\n",
    "    entry_point='inference.py',\n",
    "    framework_version='1.0-1',\n",
    "    py_version='py3'\n",
    ")\n",
    "\n",
    "print(\"üöÄ Deploying model to SageMaker endpoint...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Instance type: ml.t2.medium\")\n",
    "print(\"This will take a couple of minutes...\")\n",
    "print()\n",
    "\n",
    "# Deploy with inference script\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',\n",
    "    endpoint_name='diabetes-prediction-endpoint'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model deployed successfully!\")\n",
    "print(f\"Endpoint name: diabetes-prediction-endpoint\")\n",
    "print(f\"Endpoint URL: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e798c79-c6e1-45ec-9d26-b4786bd58712",
   "metadata": {},
   "source": [
    "## 5.1 Test Real-Time Inference\n",
    "\n",
    "Let's test our endpoint with sample patient data.\n",
    "\n",
    "**Test Cases:**\n",
    "1. Low-risk patient (normal glucose, HbA1c, BMI)\n",
    "2. High-risk patient (elevated glucose, HbA1c, BMI)\n",
    "3. Borderline patient\n",
    "\n",
    "**Input format:** [Glucose, HbA1c, BMI, Weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "54e49620-221e-4301-abf0-82bf80e58f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "# Configure predictor for CSV input/output\n",
    "predictor.serializer = CSVSerializer()\n",
    "predictor.deserializer = CSVDeserializer()\n",
    "\n",
    "print(\"‚úÖ Predictor configured for CSV format\")\n",
    "\n",
    "# Prepare test samples\n",
    "test_patient = [\n",
    "    [95, 5.5, 24.0, 70, 50, 1]\n",
    "]\n",
    "\n",
    "patient_descriptions = [\n",
    "    \"Low Risk (Normal glucose, HbA1c, BMI)\"\n",
    "]\n",
    "\n",
    "print(\"üî¨ TESTING REAL-TIME PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (patient, desc) in enumerate(zip(test_patient, patient_descriptions), 1):\n",
    "    prediction = predictor.predict([patient])[0]\n",
    "    \n",
    "    print(f\"\\nPatient {i}: {desc}\")\n",
    "    print(f\"  Input: Glucose={patient[0]}, HbA1c={patient[1]}, BMI={patient[2]}, Weight={patient[3]}\")\n",
    "    print(f\"  Prediction: {'‚ö†Ô∏è  DIABETES' if prediction == 1 else '‚úÖ NO DIABETES'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚öïÔ∏è CLINICAL NOTE:\")\n",
    "print(\"These predictions should be used as screening tools, not diagnostic tools.\")\n",
    "print(\"Always combine with clinical judgment and comprehensive testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df58224-bd58-4da6-86d2-9f4d1b895b8c",
   "metadata": {},
   "source": [
    "# Summary and Key Takeaways\n",
    "\n",
    "## What We Accomplished\n",
    "\n",
    "‚úÖ **Loaded and explored** healthcare data from DataZone  \n",
    "‚úÖ **Analyzed class imbalance** (80:1 ratio) and missing values  \n",
    "‚úÖ **Trained baseline model** locally with cross-validation  \n",
    "‚úÖ **Scaled training** using SageMaker Training Jobs  \n",
    "‚úÖ **Deployed model** to real-time endpoint  \n",
    "‚úÖ **Tested predictions** with sample patients  \n",
    "‚úÖ **Registered model** for versioning and governance  \n",
    "\n",
    "## Key Learnings\n",
    "\n",
    "### 1. Healthcare ML Challenges\n",
    "- Severe class imbalance is common in disease prediction\n",
    "- Missing data requires careful handling\n",
    "- Clinical context is crucial for model interpretation\n",
    "\n",
    "### 2. Model Performance\n",
    "- ROC-AUC: 0.9977 (excellent discrimination)\n",
    "- F1-Score: 0.82 (good balance)\n",
    "- Recall: ~76% (catches 3 out of 4 diabetes cases)\n",
    "- Trade-off between false positives and false negatives\n",
    "\n",
    "### 3. Local vs SageMaker\n",
    "- **Local**: Fast for prototyping, limited scalability\n",
    "- **SageMaker**: Production-ready, reproducible, collaborative\n",
    "\n",
    "### 4. Production Considerations\n",
    "- Use endpoints for real-time predictions\n",
    "- Model Registry for versioning and governance\n",
    "- Monitor model performance over time\n",
    "- Consider cost optimization strategies\n",
    "\n",
    "## Clinical Implications\n",
    "\n",
    "‚öïÔ∏è **Model Usage Recommendations:**\n",
    "- Use as a **screening tool**, not diagnostic tool\n",
    "- Combine with clinical judgment\n",
    "- Consider lowering threshold to catch more cases (higher recall)\n",
    "- Regular retraining as new data becomes available\n",
    "- Monitor for model drift and bias\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Immediate Actions\n",
    "1. Review model performance with clinical team\n",
    "2. Adjust threshold based on clinical priorities\n",
    "3. Set up monitoring and alerting\n",
    "4. Document model for compliance\n",
    "\n",
    "### Advanced Features (Optional)\n",
    "1. **SageMaker Clarify**: Add explainability and bias detection\n",
    "2. **SageMaker Model Monitor**: Set up drift detection\n",
    "3. **SageMaker Pipelines**: Automate the ML workflow\n",
    "4. **A/B Testing**: Compare model versions in production\n",
    "5. **DataZone Integration**: Link model metadata for governance\n",
    "\n",
    "### Production Deployment\n",
    "1. Set up CI/CD pipeline\n",
    "2. Implement model approval workflow\n",
    "3. Configure auto-scaling for endpoint\n",
    "4. Set up CloudWatch alarms\n",
    "5. Create model documentation and cards\n",
    "\n",
    "## Resources\n",
    "\n",
    "- **SageMaker Documentation**: https://docs.aws.amazon.com/sagemaker/\n",
    "- **SageMaker Examples**: https://github.com/aws/amazon-sagemaker-examples\n",
    "- **Healthcare ML on AWS**: https://aws.amazon.com/health/machine-learning/\n",
    "- **MLOps Best Practices**: https://aws.amazon.com/sagemaker/mlops/\n",
    "\n",
    "**Congratulations!** üéâ You've completed the Diabetes Prediction with SageMaker workshop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "ca50aa26-9de7-42f1-8867-68468d73bcf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
