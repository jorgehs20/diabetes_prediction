{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a7cc1d-75c7-4541-aa29-ab706de095e5",
   "metadata": {},
   "source": [
    "# üè• Diabetes Prediction with Amazon SageMaker\n",
    "\n",
    "## Workshop Context\n",
    "\n",
    "You are an **ML Engineer** who received a diabetes prediction dataset from the Data Science team through Amazon DataZone. The data was processed from FHIR healthcare records stored in AWS HealthLake.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. Understand healthcare ML workflows with imbalanced data\n",
    "2. Perform exploratory data analysis on clinical features\n",
    "3. Train a baseline model locally using scikit-learn\n",
    "4. Scale training using Amazon SageMaker Training Jobs\n",
    "5. Deploy a model to a SageMaker endpoint for real-time predictions\n",
    "6. Understand when to use local vs SageMaker training\n",
    "\n",
    "## üìä Dataset Overview\n",
    "\n",
    "**Features:**\n",
    "- `Glucose`: Blood glucose level (mg/dL)\n",
    "- `Hemoglobin A1c/Hemoglobin.total in Blood`: HbA1c percentage\n",
    "- `Body Mass Index`: BMI (kg/m¬≤)\n",
    "- `Body Weight`: Weight in kg\n",
    "- `label_diabetes_onset`: Target (0=No diabetes, 1=Diabetes)\n",
    "\n",
    "**Clinical Context:**\n",
    "- HbA1c ‚â• 6.5% indicates diabetes\n",
    "- Fasting glucose ‚â• 126 mg/dL indicates diabetes\n",
    "- Dataset has severe class imbalance (~1.2% diabetes cases)\n",
    "\n",
    "**Expected Duration:** 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e05433-15a9-4aae-992f-5cb3004f1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, f1_score, precision_recall_curve\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee369b-bef7-4a3b-8cdb-005642d4d722",
   "metadata": {},
   "source": [
    "# Section 1: Data Loading\n",
    "\n",
    "We'll load the diabetes dataset from S3. This data was published by the Data Science team through Amazon DataZone after processing FHIR data with AWS Glue.\n",
    "\n",
    "**What happens here:**\n",
    "- Connect to S3 and load Parquet files\n",
    "- Parquet format is efficient for large datasets (columnar storage, compression)\n",
    "- Display first rows to verify structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8312fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.athena\n",
    "SELECT * FROM your_database_name_here.diabetes_ml_dataset;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2195e-c3fa-4931-9e92-6612d0b24f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Replace with your S3 path\n",
    "s3_location = '<YOUR_S3_PATH_HERE>'\n",
    "\n",
    "# Read data directly\n",
    "df = pd.read_parquet(s3_location)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c426b-fb6c-4d9c-8739-9d2cd09387d1",
   "metadata": {},
   "source": [
    "# Section 2: Exploratory Data Analysis\n",
    "\n",
    "## 2.1 Class Distribution Analysis\n",
    "\n",
    "Understanding the balance between diabetes and non-diabetes cases is crucial. In healthcare, diseases are often rare, creating **class imbalance** challenges.\n",
    "\n",
    "**Why this matters:**\n",
    "- Severe imbalance affects model training\n",
    "- We need special techniques (class weighting, resampling)\n",
    "- Evaluation metrics must account for imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e919f5-f373-4d3d-a875-59265af4827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class distribution\n",
    "diabetes_count = df['label_diabetes_onset'].sum()\n",
    "total_count = len(df)\n",
    "non_diabetes_count = total_count - diabetes_count\n",
    "\n",
    "diabetes_pct = (diabetes_count / total_count) * 100\n",
    "non_diabetes_pct = (non_diabetes_count / total_count) * 100\n",
    "\n",
    "print(\"üè• CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Patients: {total_count:,}\")\n",
    "print(f\"\\n‚úÖ Non-Diabetes (Class 0): {non_diabetes_count:,} ({non_diabetes_pct:.2f}%)\")\n",
    "print(f\"‚ö†Ô∏è  Diabetes (Class 1):     {diabetes_count:,} ({diabetes_pct:.2f}%)\")\n",
    "print(f\"\\nüìä Imbalance Ratio: {non_diabetes_count/diabetes_count:.1f}:1\")\n",
    "\n",
    "print(\"\\n‚öïÔ∏è CLINICAL INSIGHT:\")\n",
    "print(\"This severe class imbalance (~80:1) is typical in disease prediction.\")\n",
    "print(\"We'll use class weighting to handle this during training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05385364-0a67-41a9-a9e2-f30f2cb0f631",
   "metadata": {},
   "source": [
    "## 2.2 Visualize Class Distribution\n",
    "\n",
    "Visual representation helps understand the scale of imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f73b0-3700-4102-a67d-67431b3f7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "ax1 = axes[0]\n",
    "counts = [non_diabetes_count, diabetes_count]\n",
    "labels = ['No Diabetes', 'Diabetes']\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "bars = ax1.bar(labels, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Number of Patients', fontsize=12)\n",
    "ax1.set_title('Class Distribution (Absolute Counts)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height):,}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "ax2 = axes[1]\n",
    "ax2.pie([non_diabetes_pct, diabetes_pct], labels=labels, autopct='%1.2f%%',\n",
    "        colors=colors, startangle=90, explode=(0, 0.1))\n",
    "ax2.set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9192d9-fed9-456e-9a33-fb08f165e261",
   "metadata": {},
   "source": [
    "## 2.3 Missing Values Analysis\n",
    "\n",
    "Missing data in healthcare can occur for many reasons:\n",
    "- Tests not ordered for certain patients\n",
    "- Lab results pending or lost\n",
    "- Data integration issues\n",
    "\n",
    "Understanding missingness patterns helps us decide on handling strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c7898-e4cd-4784-9229-570e7679a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "missing_info = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing Count': missing_values.values,\n",
    "    'Missing %': missing_percentage.values\n",
    "})\n",
    "\n",
    "missing_info = missing_info.sort_values('Missing %', ascending=False)\n",
    "missing_info = missing_info[missing_info['Missing Count'] > 0]\n",
    "\n",
    "print(\"üîç MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(missing_info) > 0:\n",
    "    print(missing_info.to_string(index=False))\n",
    "    \n",
    "    total_cells = np.product(df.shape)\n",
    "    total_missing = missing_values.sum()\n",
    "    overall_pct = (total_missing / total_cells) * 100\n",
    "    \n",
    "    print(f\"\\nüìä SUMMARY:\")\n",
    "    print(f\"Total cells: {total_cells:,}\")\n",
    "    print(f\"Total missing: {total_missing:,}\")\n",
    "    print(f\"Overall missing: {overall_pct:.2f}%\")\n",
    "    \n",
    "    print(\"\\nüí° DECISION:\")\n",
    "    print(\"Creatinine columns have >60% missing values ‚Üí DROP them\")\n",
    "    print(\"Other features have <60% missing ‚Üí Keep and handle during preprocessing\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d56aa6-6e22-4a38-9d8d-9f67ba5a42c3",
   "metadata": {},
   "source": [
    "## 2.4 Drop High-Missing Columns\n",
    "\n",
    "**Decision:** Drop Creatinine columns due to excessive missing values (>60%).\n",
    "\n",
    "**Rationale:**\n",
    "- Too many missing values make imputation unreliable\n",
    "- Creatinine is primarily a kidney function marker\n",
    "- We still have strong diabetes indicators (Glucose, HbA1c, BMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c6b4b7-a30e-4255-aa92-eb5855c22d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop creatinine columns\n",
    "columns_to_drop = [col for col in df.columns if 'creatinine' in col.lower()]\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"‚úÖ Dropped {len(columns_to_drop)} columns\")\n",
    "print(f\"üìä New shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d54592a-c429-40e5-b95d-85cdef0c360e",
   "metadata": {},
   "source": [
    "# Section 3: Local Model Training (Baseline)\n",
    "\n",
    "Now we'll train a baseline model locally using HistGradientBoostingClassifier. This gives us a performance benchmark before moving to SageMaker.\n",
    "\n",
    "**Why HistGradientBoostingClassifier?**\n",
    "- Handles missing values natively\n",
    "- Fast training on large datasets\n",
    "- Built-in class weighting for imbalanced data\n",
    "- Similar performance to XGBoost\n",
    "\n",
    "**Training Strategy:**\n",
    "- 5-fold stratified cross-validation\n",
    "- Class weighting to handle imbalance (80:1 ratio)\n",
    "- Optimize threshold using precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d05d54-562e-4585-95da-cc02751b4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def train_local_model(df):\n",
    "    \"\"\"Train baseline model locally with cross-validation\"\"\"\n",
    "    \n",
    "    # Define features\n",
    "    features = [col for col in df.columns if col not in ['patient_id', 'label_diabetes_onset']]\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df[features].apply(pd.to_numeric, errors='coerce').values\n",
    "    y = df['label_diabetes_onset'].values\n",
    "    \n",
    "    # Calculate class weights\n",
    "    n_neg = np.sum(y == 0)\n",
    "    n_pos = np.sum(y == 1)\n",
    "    weight_ratio = n_neg / n_pos\n",
    "    \n",
    "    print(\"üöÄ TRAINING LOCAL MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Features: {len(features)}\")\n",
    "    print(f\"Samples: {len(y):,}\")\n",
    "    print(f\"Class distribution: {n_neg:,} negative, {n_pos:,} positive\")\n",
    "    print(f\"Weight ratio: {weight_ratio:.2f}\")\n",
    "    print()\n",
    "    \n",
    "    # 5-fold cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(cv.split(X, y), 1):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Initialize model with class weights\n",
    "        model = HistGradientBoostingClassifier(\n",
    "            random_state=42,\n",
    "            max_iter=200,\n",
    "            class_weight={0: 1., 1: weight_ratio},\n",
    "            learning_rate=0.1\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Find optimal threshold using precision-recall curve\n",
    "        precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "        f1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1])\n",
    "        optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        \n",
    "        # Make predictions with optimal threshold\n",
    "        y_pred = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        fold_f1 = f1_score(y_test, y_pred)\n",
    "        fold_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        fold_precision = precision_score(y_test, y_pred)\n",
    "        fold_recall = recall_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"=== Fold {fold} ===\")\n",
    "        print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "        print(f\"Precision: {fold_precision:.4f}\")\n",
    "        print(f\"Recall: {fold_recall:.4f}\")\n",
    "        print(f\"F1-Score: {fold_f1:.4f}\")\n",
    "        print(f\"ROC-AUC: {fold_roc_auc:.4f}\")\n",
    "        print()\n",
    "        \n",
    "        results.append({\n",
    "            'f1': fold_f1,\n",
    "            'roc_auc': fold_roc_auc,\n",
    "            'threshold': optimal_threshold,\n",
    "            'precision': fold_precision,\n",
    "            'recall': fold_recall\n",
    "        })\n",
    "    \n",
    "    # Overall results\n",
    "    print(\"=\" * 60)\n",
    "    print(\"=== OVERALL RESULTS ===\")\n",
    "    print(f\"Average Precision: {np.mean([r['precision'] for r in results]):.4f}\")\n",
    "    print(f\"Average Recall: {np.mean([r['recall'] for r in results]):.4f}\")\n",
    "    print(f\"Average F1-Score: {np.mean([r['f1'] for r in results]):.4f}\")\n",
    "    print(f\"Average ROC-AUC: {np.mean([r['roc_auc'] for r in results]):.4f}\")\n",
    "    print(f\"Average Optimal Threshold: {np.mean([r['threshold'] for r in results]):.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Train the model\n",
    "local_results = train_local_model(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60365ef8-9846-4909-a53b-6994a2efb045",
   "metadata": {},
   "source": [
    "## Interpreting the Results\n",
    "\n",
    "**What these metrics mean:**\n",
    "\n",
    "- **ROC-AUC ~0.9997**: Outstanding! The model almost perfectly distinguishes diabetes from non-diabetes cases\n",
    "- **F1-Score ~0.92**: Excellent balance between precision and recall\n",
    "- **Optimal Threshold ~0.81**: Higher than default 0.5 due to class imbalance\n",
    "\n",
    "**Clinical Implications:**\n",
    "\n",
    "Based on your results:\n",
    "- **Recall ~91%**: Model catches 9 out of 10 diabetes cases\n",
    "- **Precision ~94%**: When model predicts diabetes, it's correct 94% of the time\n",
    "- **False Negatives**: Missing only ~9% of diabetes cases - excellent for healthcare screening\n",
    "\n",
    "**Trade-off Decision:**\n",
    "- Lower threshold ‚Üí Catch more cases (higher recall) but more false alarms\n",
    "- Higher threshold ‚Üí Fewer false alarms but miss more cases\n",
    "- For screening: Prefer higher recall (catch more cases)\n",
    "- For diagnosis: Prefer higher precision (fewer false positives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df85a5eb-9c33-490f-b2f3-a2d9048d1659",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Optional Section Notice\n",
    "\n",
    "**Sections 4 onwards (SageMaker Training Jobs + Endpoint Deployment) are optional.**\n",
    "\n",
    "These sections demonstrate:\n",
    "- Scaling ML training using Amazon SageMaker Training Jobs\n",
    "- Deploying models to SageMaker endpoints for real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff4871-b156-409e-9eff-be253c1bdcc0",
   "metadata": {},
   "source": [
    "# Section 4: SageMaker Training Job\n",
    "\n",
    "Now let's scale our training using Amazon SageMaker. This provides:\n",
    "\n",
    "**Benefits of SageMaker Training:**\n",
    "- ‚úÖ Managed infrastructure (no server management)\n",
    "- ‚úÖ Automatic experiment tracking\n",
    "- ‚úÖ Easy to scale (change instance types)\n",
    "- ‚úÖ Cost-effective (pay per minute)\n",
    "- ‚úÖ Model versioning and registry\n",
    "- ‚úÖ Production-ready artifacts\n",
    "\n",
    "**When to use SageMaker vs Local:**\n",
    "- **Local**: Quick experiments, small datasets, prototyping\n",
    "- **SageMaker**: Production models, large datasets, team collaboration, reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f2f6fb-3360-4b81-b441-f7a021d4f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'diabetes-prediction'\n",
    "\n",
    "print(\"üîß SAGEMAKER CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"S3 Bucket: {bucket}\")\n",
    "print(f\"Region: {sagemaker_session.boto_region_name}\")\n",
    "print(f\"Prefix: {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94bb4ae-9d34-4250-8ff2-b4cedd231dc1",
   "metadata": {},
   "source": [
    "## 4.1 Prepare Data for SageMaker\n",
    "\n",
    "SageMaker expects data in CSV format with:\n",
    "- First column: Target variable\n",
    "- Remaining columns: Features\n",
    "- No headers\n",
    "\n",
    "We'll split the data, save to CSV, and upload to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54776bc4-2794-462d-98d2-70bc8edd2100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "features = [col for col in df.columns if col not in ['patient_id', 'label_diabetes_onset']]\n",
    "\n",
    "# Prepare features and target\n",
    "X = df[features].apply(pd.to_numeric, errors='coerce')\n",
    "y = df['label_diabetes_onset']\n",
    "\n",
    "# Remove rows with NaN\n",
    "mask = ~X.isna().any(axis=1)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "print(f\"üìä Data after removing NaN: {len(X):,} samples\")\n",
    "\n",
    "# Split data (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")\n",
    "\n",
    "# Create CSV files (target first, then features, no headers)\n",
    "train_data = pd.concat([y_train.reset_index(drop=True), X_train.reset_index(drop=True)], axis=1)\n",
    "test_data = pd.concat([y_test.reset_index(drop=True), X_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "train_data.to_csv('train.csv', index=False, header=False)\n",
    "test_data.to_csv('test.csv', index=False, header=False)\n",
    "\n",
    "# Upload to S3\n",
    "train_s3 = sagemaker_session.upload_data('train.csv', bucket=bucket, key_prefix=f'{prefix}/train')\n",
    "test_s3 = sagemaker_session.upload_data('test.csv', bucket=bucket, key_prefix=f'{prefix}/test')\n",
    "\n",
    "print(f\"\\n‚úÖ Training data uploaded to: {train_s3}\")\n",
    "print(f\"‚úÖ Test data uploaded to: {test_s3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beec00b7-b4b9-47ae-8464-f225da582215",
   "metadata": {},
   "source": [
    "## 4.2 Create Training Script\n",
    "\n",
    "We need to create a Python script that SageMaker will execute on the training instance. This script:\n",
    "- Loads data from SageMaker's input channels\n",
    "- Trains the model\n",
    "- Saves the model to SageMaker's model directory\n",
    "\n",
    "The `%%writefile` magic command creates the file in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248dff51-ea27-404b-b4e7-291bce7c5e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_sagemaker.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score\n",
    "import joblib\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parse hyperparameters\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--max-iter', type=int, default=200)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.1)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    # Load training data\n",
    "    train_path = '/opt/ml/input/data/train'\n",
    "    train_files = [os.path.join(train_path, f) for f in os.listdir(train_path)]\n",
    "    train_data = pd.concat([pd.read_csv(f, header=None) for f in train_files])\n",
    "    \n",
    "    X_train = train_data.iloc[:, 1:].values\n",
    "    y_train = train_data.iloc[:, 0].values\n",
    "    \n",
    "    # Calculate class weights\n",
    "    n_neg = np.sum(y_train == 0)\n",
    "    n_pos = np.sum(y_train == 1)\n",
    "    weight_ratio = n_neg / n_pos\n",
    "    \n",
    "    print(f\"Training samples: {len(y_train)}\")\n",
    "    print(f\"Class distribution: {n_neg} negative, {n_pos} positive\")\n",
    "    print(f\"Weight ratio: {weight_ratio:.2f}\")\n",
    "\n",
    "    # Create sample weights\n",
    "    sample_weights = np.where(y_train == 1, weight_ratio, 1.0) \n",
    "    \n",
    "    # Train model\n",
    "    model = HistGradientBoostingClassifier(\n",
    "        random_state=42,\n",
    "        max_iter=args.max_iter,\n",
    "        learning_rate=args.learning_rate  \n",
    "    )\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    model.fit(X_train, y_train, sample_weight=sample_weights)  \n",
    "\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    # Load test data and evaluate\n",
    "    test_path = '/opt/ml/input/data/test'\n",
    "    test_files = [os.path.join(test_path, f) for f in os.listdir(test_path)]\n",
    "    test_data = pd.concat([pd.read_csv(f, header=None) for f in test_files])\n",
    "    \n",
    "    X_test = test_data.iloc[:, 1:].values\n",
    "    y_test = test_data.iloc[:, 0].values\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(\"\\n=== Model Performance ===\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join('/opt/ml/model', 'model.joblib')\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"\\nModel saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a58401a-3096-4e8f-a711-6c829df2e641",
   "metadata": {},
   "source": [
    "## 4.3 Launch SageMaker Training Job\n",
    "\n",
    "Now we'll configure and launch the training job. SageMaker will:\n",
    "1. Spin up the specified instance type\n",
    "2. Copy our training script and data\n",
    "3. Execute the training\n",
    "4. Save the model artifacts to S3\n",
    "5. Shut down the instance\n",
    "\n",
    "**Instance Selection:**\n",
    "- `ml.m5.xlarge`: Good balance of CPU/memory for this dataset\n",
    "- Training time: ~5-10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ba6c7-4dd3-4dad-a92e-c6e99b4893b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the estimator\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point='train_sagemaker.py',\n",
    "    role=role,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    framework_version='1.0-1',\n",
    "    py_version='py3',\n",
    "    hyperparameters={\n",
    "        'max-iter': 200,\n",
    "        'learning-rate': 0.1\n",
    "    },\n",
    "    base_job_name='diabetes-training'\n",
    ")\n",
    "\n",
    "print(\"üöÄ Launching SageMaker Training Job...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Instance type: ml.m5.xlarge\")\n",
    "print(f\"Framework: scikit-learn 1.0-1\")\n",
    "print(f\"Hyperparameters: max-iter=200, learning-rate=0.1\")\n",
    "print(\"\\nThis will take ~5-10 minutes...\")\n",
    "\n",
    "# Launch training job\n",
    "sklearn_estimator.fit({'train': train_s3, 'test': test_s3}, wait=True)\n",
    "\n",
    "print(\"\\n‚úÖ Training job completed!\")\n",
    "print(f\"Model artifacts saved to: {sklearn_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8816116c-2607-49cf-85a6-4f12c227661c",
   "metadata": {},
   "source": [
    "# Section 5: Model Deployment\n",
    "\n",
    "Now let's deploy our trained model to a SageMaker endpoint for real-time predictions.\n",
    "\n",
    "**What is a SageMaker Endpoint?**\n",
    "- A managed HTTPS endpoint for real-time inference\n",
    "- Auto-scaling based on traffic\n",
    "- Built-in monitoring and logging\n",
    "- A/B testing capability\n",
    "\n",
    "**Deployment Process:**\n",
    "1. SageMaker creates the endpoint configuration\n",
    "2. Deploys the model to the specified instance\n",
    "3. Endpoint becomes available for predictions\n",
    "\n",
    "**Instance Selection:**\n",
    "- `ml.t2.medium`: Cost-effective for low-traffic endpoints\n",
    "- For production: Use `ml.m5.large` or larger with auto-scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a5022-fdde-4621-9a29-f893c5e8d489",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T12:33:13.472931Z",
     "iopub.status.busy": "2025-10-23T12:33:13.472408Z",
     "iopub.status.idle": "2025-10-23T12:33:13.484494Z",
     "shell.execute_reply": "2025-10-23T12:33:13.483505Z",
     "shell.execute_reply.started": "2025-10-23T12:33:13.472904Z"
    }
   },
   "source": [
    "## Create the Inference Script\n",
    "\n",
    "The training script (`train_sagemaker.py`) only handles model training. For deployment, SageMaker needs to know:\n",
    "- How to load the saved model\n",
    "- How to process incoming prediction requests\n",
    "- How to return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d4821-84e4-41c5-abff-1c66026eb54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference.py\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO  # ‚Üê ADD THIS LINE\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load the model from the model_dir\"\"\"\n",
    "    import os\n",
    "    model_path = os.path.join(model_dir, 'model.joblib')\n",
    "    model = joblib.load(model_path)\n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"Parse input data for predictions\"\"\"\n",
    "    if request_content_type == 'text/csv':\n",
    "        # Parse CSV input\n",
    "        input_data = pd.read_csv(StringIO(request_body), header=None)\n",
    "        return input_data.values\n",
    "    elif request_content_type == 'application/json':\n",
    "        # Parse JSON input\n",
    "        import json\n",
    "        input_data = json.loads(request_body)\n",
    "        return np.array(input_data)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Make predictions using the loaded model\"\"\"\n",
    "    predictions = model.predict_proba(input_data)[:, 1]\n",
    "    return (predictions >= 0.82).astype(int)\n",
    "\n",
    "'''def predict_fn(input_data, model):\n",
    "    \"\"\"Make predictions using the loaded model\"\"\"\n",
    "    predictions = model.predict(input_data) \n",
    "    return predictions'''\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    \"\"\"Format the prediction output\"\"\"\n",
    "    if content_type == 'text/csv':  \n",
    "        return ','.join(str(int(x)) for x in prediction), content_type\n",
    "    elif content_type == 'application/json':\n",
    "        import json\n",
    "        return json.dumps(prediction.tolist()), content_type\n",
    "    else:\n",
    "        return str(prediction), content_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9f8e7-f3b8-4e95-b7c0-da7682135c35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T12:36:53.570849Z",
     "iopub.status.busy": "2025-10-23T12:36:53.570287Z",
     "iopub.status.idle": "2025-10-23T12:36:53.578586Z",
     "shell.execute_reply": "2025-10-23T12:36:53.577512Z",
     "shell.execute_reply.started": "2025-10-23T12:36:53.570821Z"
    }
   },
   "source": [
    "## Deploy the Model with Inference Script\n",
    "\n",
    "**What happens during deployment:**\n",
    "\n",
    "1. SageMaker creates a container with scikit-learn\n",
    "2. Downloads your model artifacts from S3\n",
    "3. Loads your inference script\n",
    "4. Calls `model_fn` to load the model\n",
    "5. Starts a web server to handle prediction requests\n",
    "6. Runs health checks (pings the `/ping` endpoint)\n",
    "7. Marks endpoint as \"InService\" when ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ba260-358e-42a1-9030-07ff443d7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn import SKLearnModel\n",
    "\n",
    "# Create model from training artifacts\n",
    "model = SKLearnModel(\n",
    "    model_data=sklearn_estimator.model_data,\n",
    "    role=role,\n",
    "    entry_point='inference.py',\n",
    "    framework_version='1.0-1',\n",
    "    py_version='py3'\n",
    ")\n",
    "\n",
    "print(\"üöÄ Deploying model to SageMaker endpoint...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Instance type: ml.t2.medium\")\n",
    "print(\"This will take a couple of minutes...\")\n",
    "print()\n",
    "\n",
    "# Deploy with inference script\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',\n",
    "    endpoint_name='diabetes-prediction-endpoint'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model deployed successfully!\")\n",
    "print(f\"Endpoint name: diabetes-prediction-endpoint\")\n",
    "print(f\"Endpoint URL: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e798c79-c6e1-45ec-9d26-b4786bd58712",
   "metadata": {},
   "source": [
    "## 5.1 Test Real-Time Inference\n",
    "\n",
    "Let's test our endpoint with sample patient data.\n",
    "\n",
    "**Test Cases:**\n",
    "1. Low-risk patient (normal glucose, HbA1c, BMI)\n",
    "2. High-risk patient (elevated glucose, HbA1c, BMI)\n",
    "3. Borderline patient\n",
    "\n",
    "**Input format:** [Glucose, HbA1c, BMI, Weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e49620-221e-4301-abf0-82bf80e58f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "# Configure predictor for CSV input/output\n",
    "predictor.serializer = CSVSerializer()\n",
    "predictor.deserializer = CSVDeserializer()\n",
    "\n",
    "print(\"‚úÖ Predictor configured for CSV format\")\n",
    "\n",
    "# Prepare test samples\n",
    "test_patient = [\n",
    "    [95, 5.5, 24.0, 70]\n",
    "]\n",
    "\n",
    "patient_descriptions = [\n",
    "    \"Low Risk (Normal glucose, HbA1c, BMI)\",\n",
    "    \"High Risk (Elevated glucose, HbA1c, BMI)\",\n",
    "    \"Borderline (Mixed values)\"\n",
    "]\n",
    "\n",
    "print(\"üî¨ TESTING REAL-TIME PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (patient, desc) in enumerate(zip(test_patient, patient_descriptions), 1):\n",
    "    prediction = predictor.predict([patient])[0]\n",
    "    \n",
    "    print(f\"\\nPatient {i}: {desc}\")\n",
    "    print(f\"  Input: Glucose={patient[0]}, HbA1c={patient[1]}, BMI={patient[2]}, Weight={patient[3]}\")\n",
    "    print(f\"  Prediction: {'‚ö†Ô∏è  DIABETES' if prediction == 1 else '‚úÖ NO DIABETES'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚öïÔ∏è CLINICAL NOTE:\")\n",
    "print(\"These predictions should be used as screening tools, not diagnostic tools.\")\n",
    "print(\"Always combine with clinical judgment and comprehensive testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4141663d-00ab-4f99-b221-1653a5538260",
   "metadata": {},
   "source": [
    "## 5.2 Batch Predictions\n",
    "\n",
    "For processing multiple patients at once, we can send batch requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d654f757-32b0-4557-952f-acdba197300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use test set for batch predictions\n",
    "batch_samples = X_test.iloc[:10].values  # First 10 test samples\n",
    "batch_predictions = predictor.predict(batch_samples)\n",
    "\n",
    "# Compare with actual labels\n",
    "batch_actuals = y_test.iloc[:10].values\n",
    "\n",
    "print(\"üìä BATCH PREDICTION RESULTS (First 10 patients)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Patient':<10} {'Predicted':<12} {'Actual':<10} {'Correct':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, (pred, actual) in enumerate(zip(batch_predictions, batch_actuals), 1):\n",
    "    pred_label = \"Diabetes\" if pred == 1 else \"No Diabetes\"\n",
    "    actual_label = \"Diabetes\" if actual == 1 else \"No Diabetes\"\n",
    "    correct = \"‚úÖ\" if pred == actual else \"‚ùå\"\n",
    "    print(f\"{i:<10} {pred_label:<12} {actual_label:<10} {correct:<10}\")\n",
    "\n",
    "accuracy = np.mean(batch_predictions == batch_actuals)\n",
    "print(\"-\" * 60)\n",
    "print(f\"Batch Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd70b894-c90c-4df2-829c-68828789e57f",
   "metadata": {},
   "source": [
    "# Section 6: Model Registry\n",
    "\n",
    "SageMaker Model Registry provides model versioning and governance. This is useful for:\n",
    "- Tracking model versions\n",
    "- Approval workflows\n",
    "- Integration with DataZone for governance\n",
    "- Team collaboration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb76c5-d4de-44b4-80da-1aa2ccbd2796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the model in Model Registry\n",
    "model_package_group_name = \"diabetes-prediction-models\"\n",
    "\n",
    "print(\"üì¶ Registering model in Model Registry...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_package = sklearn_estimator.register(\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=\"PendingManualApproval\",\n",
    "    description=\"Diabetes prediction model trained on FHIR healthcare data\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model registered successfully!\")\n",
    "print(f\"Model Package ARN: {model_package.model_package_arn}\")\n",
    "print(f\"Model Package Group: {model_package_group_name}\")\n",
    "print(f\"Approval Status: PendingManualApproval\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review model performance\")\n",
    "print(\"2. Approve model in SageMaker console\")\n",
    "print(\"3. Deploy approved model to production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fad521d-ddab-45ac-ba61-9f34b3ef3e75",
   "metadata": {},
   "source": [
    "# Section 7: Cleanup\n",
    "\n",
    "**Important:** Delete resources to avoid ongoing charges.\n",
    "\n",
    "‚ö†Ô∏è **Warning:** Only run this if you're done with the workshop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f907fc96-e627-4dfc-8249-5ad7c89d34d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üóëÔ∏è  Deleting endpoint...\")\n",
    "predictor.delete_endpoint()\n",
    "print(\"‚úÖ Endpoint deleted\")\n",
    "\n",
    "print(\"üóëÔ∏è  Deleting model...\")\n",
    "redictor.delete_model()\n",
    "print(\"‚úÖ Model deleted\")\n",
    "\n",
    "print(\"‚ö†Ô∏è  CLEANUP INSTRUCTIONS:\")\n",
    "print(\"1. Uncomment the lines above to delete endpoint and model\")\n",
    "print(\"2. Or delete manually in SageMaker console\")\n",
    "print(\"3. Training artifacts in S3 will remain (minimal cost)\")\n",
    "print(\"4. Delete S3 objects if needed: s3://{}/{}\".format(bucket, prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df58224-bd58-4da6-86d2-9f4d1b895b8c",
   "metadata": {},
   "source": [
    "# Summary and Key Takeaways\n",
    "\n",
    "## What We Accomplished\n",
    "\n",
    "‚úÖ **Loaded and explored** healthcare data from DataZone  \n",
    "‚úÖ **Analyzed class imbalance** (80:1 ratio) and missing values  \n",
    "‚úÖ **Trained baseline model** locally with cross-validation  \n",
    "‚úÖ **Scaled training** using SageMaker Training Jobs  \n",
    "‚úÖ **Deployed model** to real-time endpoint  \n",
    "‚úÖ **Tested predictions** with sample patients  \n",
    "‚úÖ **Registered model** for versioning and governance  \n",
    "\n",
    "## Key Learnings\n",
    "\n",
    "### 1. Healthcare ML Challenges\n",
    "- Severe class imbalance is common in disease prediction\n",
    "- Missing data requires careful handling\n",
    "- Clinical context is crucial for model interpretation\n",
    "\n",
    "### 2. Model Performance\n",
    "- ROC-AUC: 0.9977 (excellent discrimination)\n",
    "- F1-Score: 0.79 (good balance)\n",
    "- Recall: ~73% (catches 3 out of 4 diabetes cases)\n",
    "- Trade-off between false positives and false negatives\n",
    "\n",
    "### 3. Local vs SageMaker\n",
    "- **Local**: Fast for prototyping, limited scalability\n",
    "- **SageMaker**: Production-ready, reproducible, collaborative\n",
    "\n",
    "### 4. Production Considerations\n",
    "- Use endpoints for real-time predictions\n",
    "- Model Registry for versioning and governance\n",
    "- Monitor model performance over time\n",
    "- Consider cost optimization strategies\n",
    "\n",
    "## Clinical Implications\n",
    "\n",
    "‚öïÔ∏è **Model Usage Recommendations:**\n",
    "- Use as a **screening tool**, not diagnostic tool\n",
    "- Combine with clinical judgment\n",
    "- Consider lowering threshold to catch more cases (higher recall)\n",
    "- Regular retraining as new data becomes available\n",
    "- Monitor for model drift and bias\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Immediate Actions\n",
    "1. Review model performance with clinical team\n",
    "2. Adjust threshold based on clinical priorities\n",
    "3. Set up monitoring and alerting\n",
    "4. Document model for compliance\n",
    "\n",
    "### Advanced Features (Optional)\n",
    "1. **SageMaker Clarify**: Add explainability and bias detection\n",
    "2. **SageMaker Model Monitor**: Set up drift detection\n",
    "3. **SageMaker Pipelines**: Automate the ML workflow\n",
    "4. **A/B Testing**: Compare model versions in production\n",
    "5. **DataZone Integration**: Link model metadata for governance\n",
    "\n",
    "### Production Deployment\n",
    "1. Set up CI/CD pipeline\n",
    "2. Implement model approval workflow\n",
    "3. Configure auto-scaling for endpoint\n",
    "4. Set up CloudWatch alarms\n",
    "5. Create model documentation and cards\n",
    "\n",
    "## Resources\n",
    "\n",
    "- **SageMaker Documentation**: https://docs.aws.amazon.com/sagemaker/\n",
    "- **SageMaker Examples**: https://github.com/aws/amazon-sagemaker-examples\n",
    "- **Healthcare ML on AWS**: https://aws.amazon.com/health/machine-learning/\n",
    "- **MLOps Best Practices**: https://aws.amazon.com/sagemaker/mlops/\n",
    "\n",
    "**Congratulations!** üéâ You've completed the Diabetes Prediction with SageMaker workshop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca50aa26-9de7-42f1-8867-68468d73bcf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
